{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cf29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# %pip install langdetect pycld2 lingua-language-detector fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e180ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "import langdetect\n",
    "import pycld2\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import fasttext\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe9ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LanguageDetectionResult dataclass\n",
    "@dataclass\n",
    "class LanguageDetectionResult:\n",
    "    \"\"\"Result of language detection operation.\"\"\"\n",
    "    language_code: str\n",
    "    confidence: float\n",
    "    source: str  # 'langdetect', 'pycld2', 'lingua', 'whisper', etc.\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate the result after initialization.\"\"\"\n",
    "        if not isinstance(self.language_code, str) or len(self.language_code) < 2:\n",
    "            raise ValueError(\"language_code must be a valid string\")\n",
    "        if not (0.0 <= self.confidence <= 1.0):\n",
    "            raise ValueError(\"confidence must be between 0.0 and 1.0\")\n",
    "        if not isinstance(self.source, str):\n",
    "            raise ValueError(\"source must be a string\")\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert result to dictionary.\"\"\"\n",
    "        return {\n",
    "            'language_code': self.language_code,\n",
    "            'confidence': self.confidence,\n",
    "            'source': self.source\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'LanguageDetectionResult':\n",
    "        \"\"\"Create result from dictionary.\"\"\"\n",
    "        return cls(\n",
    "            language_code=data['language_code'],\n",
    "            confidence=data['confidence'],\n",
    "            source=data['source']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8eedd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define language code normalization function\n",
    "def normalize_language_code(raw_code: str) -> str:\n",
    "    \"\"\"Normalize language codes to standard ISO 639-1 format.\n",
    "    \n",
    "    Args:\n",
    "        raw_code: Raw language code from various providers\n",
    "        \n",
    "    Returns:\n",
    "        Normalized ISO 639-1 language code (e.g., 'en', 'es', 'fr')\n",
    "    \"\"\"\n",
    "    # Common mappings from various providers to ISO 639-1\n",
    "    normalization_map = {\n",
    "        # langdetect mappings\n",
    "        'en': 'en', 'english': 'en',\n",
    "        'es': 'es', 'spanish': 'es', \n",
    "        'fr': 'fr', 'french': 'fr',\n",
    "        'de': 'de', 'german': 'de',\n",
    "        'it': 'it', 'italian': 'it',\n",
    "        'pt': 'pt', 'portuguese': 'pt',\n",
    "        'ru': 'ru', 'russian': 'ru',\n",
    "        'ja': 'ja', 'japanese': 'ja',\n",
    "        'ko': 'ko', 'korean': 'ko',\n",
    "        'zh': 'zh', 'chinese': 'zh',\n",
    "        'ar': 'ar', 'arabic': 'ar',\n",
    "        'hi': 'hi', 'hindi': 'hi',\n",
    "        \n",
    "        # pycld2 mappings (often returns full names)\n",
    "        'ENGLISH': 'en',\n",
    "        'SPANISH': 'es',\n",
    "        'FRENCH': 'fr',\n",
    "        'GERMAN': 'de',\n",
    "        'ITALIAN': 'it',\n",
    "        'PORTUGUESE': 'pt',\n",
    "        'RUSSIAN': 'ru',\n",
    "        'JAPANESE': 'ja',\n",
    "        'KOREAN': 'ko',\n",
    "        'CHINESE': 'zh',\n",
    "        'ARABIC': 'ar',\n",
    "        'HINDI': 'hi',\n",
    "        \n",
    "        # Lingua mappings (Language enum values)\n",
    "        'Language.ENGLISH': 'en',\n",
    "        'Language.SPANISH': 'es',\n",
    "        'Language.FRENCH': 'fr',\n",
    "        'Language.GERMAN': 'de',\n",
    "        'Language.ITALIAN': 'it',\n",
    "        'Language.PORTUGUESE': 'pt',\n",
    "        'Language.RUSSIAN': 'ru',\n",
    "        'Language.JAPANESE': 'ja',\n",
    "        'Language.KOREAN': 'ko',\n",
    "        'Language.CHINESE': 'zh',\n",
    "        'Language.ARABIC': 'ar',\n",
    "        'Language.HINDI': 'hi',\n",
    "        \n",
    "        # Whisper mappings (ISO 639-3 to ISO 639-1)\n",
    "        'eng': 'en',\n",
    "        'spa': 'es',\n",
    "        'fra': 'fr',\n",
    "        'deu': 'de',\n",
    "        'ita': 'it',\n",
    "        'por': 'pt',\n",
    "        'rus': 'ru',\n",
    "        'jpn': 'ja',\n",
    "        'kor': 'ko',\n",
    "        'zho': 'zh',\n",
    "        'ara': 'ar',\n",
    "        'hin': 'hi',\n",
    "    }\n",
    "    \n",
    "    # Clean and normalize the input\n",
    "    cleaned_code = raw_code.strip().upper()\n",
    "    \n",
    "    # Direct mapping\n",
    "    if cleaned_code in normalization_map:\n",
    "        return normalization_map[cleaned_code]\n",
    "    \n",
    "    # Try lowercase version\n",
    "    lower_code = cleaned_code.lower()\n",
    "    if lower_code in normalization_map:\n",
    "        return normalization_map[lower_code]\n",
    "    \n",
    "    # Try first two characters for ISO codes\n",
    "    if len(raw_code) >= 2:\n",
    "        prefix = raw_code[:2].lower()\n",
    "        if prefix in normalization_map:\n",
    "            return normalization_map[prefix]\n",
    "    \n",
    "    # Return original if no mapping found\n",
    "    return raw_code.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba112aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LanguageDetector class\n",
    "class LanguageDetector:\n",
    "    \"\"\"Multi-provider language detection with fallback support.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize language detectors.\"\"\"\n",
    "        self._detectors = {}\n",
    "        self._initialize_detectors()\n",
    "    \n",
    "    def _initialize_detectors(self):\n",
    "        \"\"\"Initialize available language detection providers.\"\"\"\n",
    "        try:\n",
    "            # langdetect\n",
    "            self._detectors['langdetect'] = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: langdetect not available: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # pycld2\n",
    "            self._detectors['pycld2'] = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: pycld2 not available: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # Lingua\n",
    "            languages = [Language.ENGLISH, Language.SPANISH, Language.FRENCH, \n",
    "                        Language.GERMAN, Language.ITALIAN, Language.PORTUGUESE,\n",
    "                        Language.RUSSIAN, Language.JAPANESE, Language.KOREAN,\n",
    "                        Language.CHINESE, Language.ARABIC, Language.HINDI]\n",
    "            self._lingua_detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "            self._detectors['lingua'] = True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Lingua not available: {e}\")\n",
    "    \n",
    "    def detect_from_text(self, text: str) -> LanguageDetectionResult:\n",
    "        \"\"\"Detect language from text using multiple providers with fallback.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to detect language for\n",
    "            \n",
    "        Returns:\n",
    "            LanguageDetectionResult with detected language, confidence, and source\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return LanguageDetectionResult('unknown', 0.0, 'empty_input')\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Try langdetect first\n",
    "        if 'langdetect' in self._detectors:\n",
    "            try:\n",
    "                lang = langdetect.detect(text)\n",
    "                confidence = langdetect.detect_langs(text)[0].prob\n",
    "                results.append(LanguageDetectionResult(\n",
    "                    normalize_language_code(lang), \n",
    "                    confidence, \n",
    "                    'langdetect'\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"langdetect failed: {e}\")\n",
    "        \n",
    "        # Try pycld2\n",
    "        if 'pycld2' in self._detectors:\n",
    "            try:\n",
    "                is_reliable, text_bytes_found, details = pycld2.detect(text)\n",
    "                if is_reliable and details:\n",
    "                    lang_code = details[0][1]\n",
    "                    confidence = details[0][2] / 100.0  # Convert to 0-1 scale\n",
    "                    results.append(LanguageDetectionResult(\n",
    "                        normalize_language_code(lang_code),\n",
    "                        confidence,\n",
    "                        'pycld2'\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                print(f\"pycld2 failed: {e}\")\n",
    "        \n",
    "        # Try Lingua\n",
    "        if 'lingua' in self._detectors:\n",
    "            try:\n",
    "                confidence_values = self._lingua_detector.compute_language_confidence_values(text)\n",
    "                if confidence_values:\n",
    "                    best_match = confidence_values[0]\n",
    "                    lang_code = str(best_match.language).split('.')[-1].lower()\n",
    "                    results.append(LanguageDetectionResult(\n",
    "                        normalize_language_code(lang_code),\n",
    "                        best_match.value,\n",
    "                        'lingua'\n",
    "                    ))\n",
    "            except Exception as e:\n",
    "                print(f\"Lingua failed: {e}\")\n",
    "        \n",
    "        # Return best result or fallback\n",
    "        if results:\n",
    "            # Sort by confidence and return highest\n",
    "            results.sort(key=lambda x: x.confidence, reverse=True)\n",
    "            return results[0]\n",
    "        else:\n",
    "            return LanguageDetectionResult('unknown', 0.0, 'no_providers')\n",
    "    \n",
    "    def detect_from_audio(self, audio_bytes: bytes) -> Optional[LanguageDetectionResult]:\n",
    "        \"\"\"Detect language from audio bytes using Whisper.\n",
    "        \n",
    "        Note: This requires the STT module to be available.\n",
    "        For now, returns None as we need to integrate with STT.\n",
    "        \n",
    "        Args:\n",
    "            audio_bytes: Audio data in bytes\n",
    "            \n",
    "        Returns:\n",
    "            LanguageDetectionResult or None if detection fails\n",
    "        \"\"\"\n",
    "        # This would integrate with the STT module's language detection\n",
    "        # For now, return None to indicate audio detection not implemented\n",
    "        # In a full implementation, this would call the STT module\n",
    "        return None\n",
    "    \n",
    "    def get_available_providers(self) -> List[str]:\n",
    "        \"\"\"Get list of available language detection providers.\"\"\"\n",
    "        return list(self._detectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33d22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LanguagePreferenceManager class\n",
    "class LanguagePreferenceManager:\n",
    "    \"\"\"Manages language preferences per session/user.\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_file: str = \"language_preferences.json\"):\n",
    "        \"\"\"Initialize preference manager.\n",
    "        \n",
    "        Args:\n",
    "            storage_file: File to store preferences (JSON format)\n",
    "        \"\"\"\n",
    "        self.storage_file = storage_file\n",
    "        self.preferences = self._load_preferences()\n",
    "    \n",
    "    def _load_preferences(self) -> Dict:\n",
    "        \"\"\"Load preferences from storage file.\"\"\"\n",
    "        if os.path.exists(self.storage_file):\n",
    "            try:\n",
    "                with open(self.storage_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load preferences: {e}\")\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_preferences(self):\n",
    "        \"\"\"Save preferences to storage file.\"\"\"\n",
    "        try:\n",
    "            with open(self.storage_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.preferences, f, indent=2, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save preferences: {e}\")\n",
    "    \n",
    "    def set_user_preference(self, user_id: str, language_code: str, confidence: float = 1.0):\n",
    "        \"\"\"Set language preference for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: Unique identifier for the user/session\n",
    "            language_code: Preferred language code\n",
    "            confidence: Confidence in this preference (0.0-1.0)\n",
    "        \"\"\"\n",
    "        if user_id not in self.preferences:\n",
    "            self.preferences[user_id] = {}\n",
    "        \n",
    "        self.preferences[user_id][language_code] = {\n",
    "            'confidence': confidence,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'count': self.preferences[user_id].get(language_code, {}).get('count', 0) + 1\n",
    "        }\n",
    "        \n",
    "        self._save_preferences()\n",
    "    \n",
    "    def get_user_preference(self, user_id: str) -> Optional[str]:\n",
    "        \"\"\"Get the most preferred language for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: Unique identifier for the user/session\n",
    "            \n",
    "        Returns:\n",
    "            Most preferred language code or None if no preferences\n",
    "        \"\"\"\n",
    "        if user_id not in self.preferences:\n",
    "            return None\n",
    "        \n",
    "        user_prefs = self.preferences[user_id]\n",
    "        if not user_prefs:\n",
    "            return None\n",
    "        \n",
    "        # Find language with highest combined score (confidence * count)\n",
    "        best_lang = max(user_prefs.items(), \n",
    "                       key=lambda x: x[1]['confidence'] * x[1]['count'])\n",
    "        return best_lang[0]\n",
    "    \n",
    "    def update_from_detection(self, user_id: str, detection_result: LanguageDetectionResult):\n",
    "        \"\"\"Update user preferences based on language detection result.\n",
    "        \n",
    "        Args:\n",
    "            user_id: Unique identifier for the user/session\n",
    "            detection_result: Result from language detection\n",
    "        \"\"\"\n",
    "        self.set_user_preference(user_id, detection_result.language_code, \n",
    "                               detection_result.confidence)\n",
    "    \n",
    "    def get_user_history(self, user_id: str) -> Dict:\n",
    "        \"\"\"Get complete language preference history for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: Unique identifier for the user/session\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of language preferences with metadata\n",
    "        \"\"\"\n",
    "        return self.preferences.get(user_id, {}).copy()\n",
    "    \n",
    "    def clear_user_preferences(self, user_id: str):\n",
    "        \"\"\"Clear all preferences for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: Unique identifier for the user/session\n",
    "        \"\"\"\n",
    "        if user_id in self.preferences:\n",
    "            del self.preferences[user_id]\n",
    "            self._save_preferences()\n",
    "    \n",
    "    def get_all_users(self) -> List[str]:\n",
    "        \"\"\"Get list of all users with preferences.\"\"\"\n",
    "        return list(self.preferences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b16bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageDetectionResult created successfully:\n",
      "Language: en\n",
      "Confidence: 0.95\n",
      "Source: langdetect\n",
      "As dict: {'language_code': 'en', 'confidence': 0.95, 'source': 'langdetect'}\n"
     ]
    }
   ],
   "source": [
    "# Test LanguageDetectionResult\n",
    "try:\n",
    "    result = LanguageDetectionResult('en', 0.95, 'langdetect')\n",
    "    print(\"LanguageDetectionResult created successfully:\")\n",
    "    print(f\"Language: {result.language_code}\")\n",
    "    print(f\"Confidence: {result.confidence}\")\n",
    "    print(f\"Source: {result.source}\")\n",
    "    print(f\"As dict: {result.to_dict()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating LanguageDetectionResult: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4958161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language code normalization:\n",
      "'en' -> 'en'\n",
      "'ENGLISH' -> 'en'\n",
      "'eng' -> 'en'\n",
      "'Language.ENGLISH' -> 'language.english'\n",
      "'spa' -> 'es'\n",
      "'SPANISH' -> 'es'\n",
      "'fra' -> 'fr'\n",
      "'zh' -> 'zh'\n",
      "'unknown' -> 'unknown'\n"
     ]
    }
   ],
   "source": [
    "# Test normalize_language_code function\n",
    "test_codes = ['en', 'ENGLISH', 'eng', 'Language.ENGLISH', 'spa', 'SPANISH', 'fra', 'zh', 'unknown']\n",
    "print(\"Testing language code normalization:\")\n",
    "for code in test_codes:\n",
    "    normalized = normalize_language_code(code)\n",
    "    print(f\"'{code}' -> '{normalized}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42788008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageDetector initialized successfully.\n",
      "Available providers: ['langdetect', 'pycld2', 'lingua']\n"
     ]
    }
   ],
   "source": [
    "# Test LanguageDetector class instantiation\n",
    "try:\n",
    "    detector = LanguageDetector()\n",
    "    print(\"LanguageDetector initialized successfully.\")\n",
    "    print(f\"Available providers: {detector.get_available_providers()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LanguageDetector: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee27e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language detection from text:\n",
      "Text: 'Hello, how are you today?...' -> en (0.96) via pycld2\n",
      "Text: 'Hola, ¿cómo estás hoy?...' -> es (0.94) via lingua\n",
      "Text: 'Bonjour, comment allez-vous?...' -> fr (1.00) via langdetect\n",
      "Text: 'Hallo, wie geht es dir?...' -> de (0.95) via pycld2\n",
      "Text: 'こんにちは、今日はどうですか？...' -> ja (1.00) via lingua\n",
      "Text: '안녕하세요, 오늘 어떻게 지내세요?...' -> ko (1.00) via lingua\n"
     ]
    }
   ],
   "source": [
    "# Test text language detection\n",
    "test_texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Hola, ¿cómo estás hoy?\",\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Hallo, wie geht es dir?\",\n",
    "    \"こんにちは、今日はどうですか？\",\n",
    "    \"안녕하세요, 오늘 어떻게 지내세요?\"\n",
    "]\n",
    "\n",
    "print(\"Testing language detection from text:\")\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        result = detector.detect_from_text(text)\n",
    "        print(f\"Text: '{text[:30]}...' -> {result.language_code} ({result.confidence:.2f}) via {result.source}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting language for '{text[:30]}...': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e834475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguagePreferenceManager initialized successfully.\n",
      "User preference: en\n",
      "User history: {'en': {'confidence': 0.95, 'timestamp': '2025-11-21T22:00:51.339325', 'count': 4}, 'es': {'confidence': 0.8, 'timestamp': '2025-11-21T22:00:51.338250', 'count': 2}}\n"
     ]
    }
   ],
   "source": [
    "# Test LanguagePreferenceManager\n",
    "try:\n",
    "    pref_manager = LanguagePreferenceManager()\n",
    "    print(\"LanguagePreferenceManager initialized successfully.\")\n",
    "    \n",
    "    # Test setting and getting preferences\n",
    "    user_id = \"test_user\"\n",
    "    pref_manager.set_user_preference(user_id, \"en\", 0.9)\n",
    "    pref_manager.set_user_preference(user_id, \"es\", 0.8)\n",
    "    pref_manager.set_user_preference(user_id, \"en\", 0.95)  # Update English preference\n",
    "    \n",
    "    preference = pref_manager.get_user_preference(user_id)\n",
    "    print(f\"User preference: {preference}\")\n",
    "    \n",
    "    history = pref_manager.get_user_history(user_id)\n",
    "    print(f\"User history: {history}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing LanguagePreferenceManager: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff56d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection result: en (1.00)\n",
      "Updated preference: en\n"
     ]
    }
   ],
   "source": [
    "# Test integration with detection results\n",
    "try:\n",
    "    # Detect language and update preferences\n",
    "    test_text = \"This is a test sentence in English.\"\n",
    "    result = detector.detect_from_text(test_text)\n",
    "    print(f\"Detection result: {result.language_code} ({result.confidence:.2f})\")\n",
    "    \n",
    "    # Update preferences\n",
    "    pref_manager.update_from_detection(\"integration_test\", result)\n",
    "    pref = pref_manager.get_user_preference(\"integration_test\")\n",
    "    print(f\"Updated preference: {pref}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in integration test: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
